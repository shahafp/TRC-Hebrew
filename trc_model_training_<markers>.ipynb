{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from trc_model.temporal_relation_classification import TemporalRelationClassification\n",
    "from trc_model.temporal_relation_classification_config import TemporalRelationClassificationConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, \\\n",
    "    AutoModelForSequenceClassification, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/guy.yanko/.cache/huggingface/datasets/csv/new_markers_data-c1b27f3938300914/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddd8462dd1f745a584ad352a12e6add7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'text', 'label', 'named_label'],\n        num_rows: 5826\n    })\n    test: Dataset({\n        features: ['Unnamed: 0', 'text', 'label', 'named_label'],\n        num_rows: 1434\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"data_handling/new_markers_data\")\n",
    "raw_datasets\n",
    "# raw_datasets = load_dataset(\"guyyanko/trc-hebrew-no-special-markers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {}\n",
    "id2label = {}\n",
    "for label, named_label in zip(raw_datasets['train']['label'], raw_datasets['train']['named_label']):\n",
    "    label2id[named_label] = label\n",
    "    id2label[label] = named_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mode = False\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    if eval_mode:\n",
    "        report = classification_report(y_true=labels, y_pred=predictions,\n",
    "                                       target_names=['BEFORE', 'AFTER', 'EQUAL', 'VAGUE'])\n",
    "        with open(f'{model_final_name}/evaluation_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "        print(report)\n",
    "\n",
    "    results = \\\n",
    "        classification_report(y_true=labels, y_pred=predictions, target_names=['BEFORE', 'AFTER', 'EQUAL', 'VAGUE'],\n",
    "                              output_dict=True)['weighted avg']\n",
    "    results.pop('support')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_checkpoints = ['onlplab/alephbert-base', 'avichr/heBERT', 'imvladikon/alephbertgimmel-base-512']\n",
    "architectures = ['SEQ_CLS', 'ESS', 'EMP', 'EF'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/new_markers_data-c1b27f3938300914/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-daee9656cddf3b45.arrow\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/new_markers_data-c1b27f3938300914/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-09f018e90fcf17af.arrow\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 20\u001B[0m\n\u001B[1;32m     11\u001B[0m config \u001B[38;5;241m=\u001B[39m TemporalRelationClassificationConfig(ES_ID\u001B[38;5;241m=\u001B[39mES_ID,\n\u001B[1;32m     12\u001B[0m                                               architecture\u001B[38;5;241m=\u001B[39marc,\n\u001B[1;32m     13\u001B[0m                                               num_labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(label2id),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m                                               name_or_path\u001B[38;5;241m=\u001B[39mcheckpoint,\n\u001B[1;32m     17\u001B[0m                                               tokenizer_class\u001B[38;5;241m=\u001B[39mtokenizer_class)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=checkpoint)\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mTemporalRelationClassification\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# model.bert.resize_token_embeddings(len(tokenizer))\u001B[39;00m\n\u001B[1;32m     23\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     24\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39mmodel_final_name,\n\u001B[1;32m     25\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     32\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[],\n\u001B[1;32m     33\u001B[0m )\n",
      "File \u001B[0;32m~/Master/TRC-Hebrew/trc_model/temporal_relation_classification.py:25\u001B[0m, in \u001B[0;36mTemporalRelationClassification.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# self.bert = BertModel(config)\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert \u001B[38;5;241m=\u001B[39m \u001B[43mBertModel\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(config\u001B[38;5;241m.\u001B[39mname_or_path)\n\u001B[1;32m     26\u001B[0m classifier_dropout \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     27\u001B[0m     config\u001B[38;5;241m.\u001B[39mclassifier_dropout \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mclassifier_dropout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m config\u001B[38;5;241m.\u001B[39mhidden_dropout_prob\n\u001B[1;32m     28\u001B[0m )\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(classifier_dropout)\n",
      "File \u001B[0;32m~/Master/TRC-Hebrew/trc_model/temporal_relation_classification.py:25\u001B[0m, in \u001B[0;36mTemporalRelationClassification.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# self.bert = BertModel(config)\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert \u001B[38;5;241m=\u001B[39m \u001B[43mBertModel\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(config\u001B[38;5;241m.\u001B[39mname_or_path)\n\u001B[1;32m     26\u001B[0m classifier_dropout \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     27\u001B[0m     config\u001B[38;5;241m.\u001B[39mclassifier_dropout \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mclassifier_dropout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m config\u001B[38;5;241m.\u001B[39mhidden_dropout_prob\n\u001B[1;32m     28\u001B[0m )\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(classifier_dropout)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for checkpoint in lm_checkpoints:\n",
    "    for arc in architectures:\n",
    "        model_final_name = f'hebrew-trc-{checkpoint.split(\"/\")[1]}-{arc}'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        # tokenizer.add_special_tokens({'additional_special_tokens': ['[א1]', '[/א1]', '[א2]', '[/א2]']})\n",
    "        ES_ID = tokenizer.convert_tokens_to_ids('<')\n",
    "        tokenized_datasets = raw_datasets.map(preprocess_function, remove_columns=['named_label'], batched=True)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "        tokenizer_class = str(type(tokenizer)).strip(\"><'\").split('.')[-1]\n",
    "        config = TemporalRelationClassificationConfig(ES_ID=ES_ID,\n",
    "                                                      architecture=arc,\n",
    "                                                      num_labels=len(label2id),\n",
    "                                                      id2label=id2label,\n",
    "                                                      label2id=label2id,\n",
    "                                                      name_or_path=checkpoint,\n",
    "                                                      tokenizer_class=tokenizer_class)\n",
    "\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=checkpoint)\n",
    "        model = TemporalRelationClassification(config=config)\n",
    "        # model.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_final_name,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            weight_decay=0.01,\n",
    "            num_train_epochs=20,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            report_to=[],\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"].shuffle(),\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        eval_mode = True\n",
    "        print('Evaluate:', model_final_name)\n",
    "        trainer.evaluate(tokenized_datasets['test'])\n",
    "        eval_mode = False\n",
    "        config.register_for_auto_class()\n",
    "        model.register_for_auto_class('AutoModelForSequenceClassification')\n",
    "        # trainer.push_to_hub()\n",
    "        trainer.save_model(model_final_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
