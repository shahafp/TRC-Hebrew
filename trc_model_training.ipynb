{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from trc_model import TemporalRelationClassification, TemporalRelationClassificationConfig\n",
    "from transformers import TrainingArguments, Trainer, AutoConfig, AutoModelForSequenceClassification, \\\n",
    "    DataCollatorWithPadding, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AutoConfig.register(\"TemporalRelationClassification\", TemporalRelationClassificationConfig)\n",
    "AutoModelForSequenceClassification.register(TemporalRelationClassificationConfig, TemporalRelationClassification)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"guyyanko/trc-hebrew\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label2id = {}\n",
    "id2label = {}\n",
    "for label, named_label in zip(raw_datasets['train']['label'], raw_datasets['train']['named_label']):\n",
    "    label2id[named_label] = label\n",
    "    id2label[label] = named_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "eval_mode = False\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    true_labels = [id2label[label] for label in labels]\n",
    "    true_predictions = [id2label[pred] for pred in predictions]\n",
    "    if eval_mode:\n",
    "        print(classification_report(y_true=true_labels, y_pred=true_predictions))\n",
    "    all_metrics = classification_report(y_true=true_labels, y_pred=true_predictions, output_dict=True)['weighted avg']\n",
    "    all_metrics.pop('support')\n",
    "    return all_metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm_checkpoints = ['onlplab/alephbert-base', 'avichr/heBERT', 'imvladikon/alephbertgimmel-base-512']\n",
    "architectures = ['SEQ_CLS', 'ESS', 'EMP', 'EF']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for checkpoint in lm_checkpoints:\n",
    "    for arc in architectures:\n",
    "        model_final_name = f'hebrew-trc-{checkpoint.split(\"/\")[1]}-{arc}'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        tokenizer.add_tokens(['[א1]', '[/א1]', '[א2]', '[/א2]'])\n",
    "        E1_start = tokenizer.convert_tokens_to_ids('[א1]')\n",
    "        E2_start = tokenizer.convert_tokens_to_ids('[א2]')\n",
    "        tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "        tokenizer_class = str(type(tokenizer)).strip(\"><'\").split('.')[-1]\n",
    "        config = TemporalRelationClassificationConfig(EMS1=E1_start, EMS2=E2_start, architecture=arc,\n",
    "                                                      token_embeddings_size=len(tokenizer), num_labels=len(label2id),\n",
    "                                                      id2label=id2label,\n",
    "                                                      label2id=label2id, name_or_path=checkpoint,\n",
    "                                                      tokenizer_class=tokenizer_class)\n",
    "\n",
    "        model = TemporalRelationClassification(config=config)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_final_name,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            num_train_epochs=15,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=8,\n",
    "            save_strategy=\"no\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        eval_mode = True\n",
    "        print('Evaluate:', model_final_name)\n",
    "        trainer.evaluate(tokenized_datasets['test'])\n",
    "        eval_mode = False\n",
    "        config.register_for_auto_class()\n",
    "        model.register_for_auto_class('AutoModelForSequenceClassification')\n",
    "        trainer.push_to_hub()\n",
    "        trainer.save_model(model_final_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# tokenizer.add_tokens(['[א1]', '[/א1]', '[א2]', '[/א2]'])\n",
    "# E1_start = tokenizer.convert_tokens_to_ids('[א1]')\n",
    "# E2_start = tokenizer.convert_tokens_to_ids('[א2]')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenized_datasets = DatasetBuilder.tokenize_dataset(raw_datasets, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# config = TemporalRelationClassificationConfig(EMS1=E1_start, EMS2=E2_start, architecture='EMP',\n",
    "#                                               token_embeddings_size=len(tokenizer), num_labels=len(LABELS),\n",
    "#                                               id2label=id2label,\n",
    "#                                               label2id=label2id, name_or_path=model_checkpoint,\n",
    "#                                               tokenizer_class='BertTokenizerFast')\n",
    "#\n",
    "# model = TemporalRelationClassification(config=config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"trc-model-emp\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     num_train_epochs=15,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"no\",\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.save_model('my_trc')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from trc_pipeline import TemporalRelationClassificationPipeline\n",
    "# from transformers.pipelines import PIPELINE_REGISTRY\n",
    "#\n",
    "# PIPELINE_REGISTRY.register_pipeline(\n",
    "#     \"temporal-relation-classification\",\n",
    "#     pipeline_class=TemporalRelationClassificationPipeline,\n",
    "#     pt_model=TemporalRelationClassification,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "#\n",
    "# classifier = pipeline(\"temporal-relation-classification\", model=\"my_trc\", trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# classifier(\n",
    "#     \"מקורות פלשתיניים [א1] מסרו [/א1] כי חיילים, שירדו ממיניבוס לבן בשכונת ראפידיה בשכם, [א2] ניפצו [/א2] בעזרת אבנים זגוגיות של מכוניות חונות של תושבים מקומיים, ביניהן גם את חלונות מכוניתו של דר מוסטפא מקבול.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import Repository\n",
    "#\n",
    "# repo = Repository(\"trc-emp-pipeline\", clone_from=\"guyyanko/trc-model-emp\")\n",
    "# classifier.save_pretrained(\"trc-emp-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# repo.push_to_hub()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
