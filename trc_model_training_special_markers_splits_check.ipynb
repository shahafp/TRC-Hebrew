{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from trc_model.temporal_relation_classification import TemporalRelationClassification\n",
    "from trc_model.temporal_relation_classification_config import TemporalRelationClassificationConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mode = False\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    if eval_mode:\n",
    "        report = classification_report(y_true=labels, y_pred=predictions,\n",
    "                                       target_names=['BEFORE', 'AFTER', 'EQUAL', 'VAGUE'])\n",
    "        for i in range(labels.shape[0]):\n",
    "            if labels[i] == 3 and predictions[i] != 3:\n",
    "                labels[i] = predictions[i]\n",
    "        report_no_vague = classification_report(y_true=labels, y_pred=predictions,\n",
    "                                                target_names=['BEFORE', 'AFTER', 'EQUAL', 'VAGUE'])\n",
    "\n",
    "        with open(f'{model_final_name}/evaluation_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "            f.write('\\n')\n",
    "            f.write(report_no_vague)\n",
    "        print(report)\n",
    "        print(report_no_vague)\n",
    "\n",
    "    results = \\\n",
    "        classification_report(y_true=labels, y_pred=predictions, target_names=['BEFORE', 'AFTER', 'EQUAL', 'VAGUE'],\n",
    "                              output_dict=True)['weighted avg']\n",
    "    results.pop('support')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_checkpoints = ['onlplab/alephbert-base']\n",
    "architectures = ['EMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/guy.yanko/.cache/huggingface/datasets/csv/split_1-342133e3b8689d17/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01067a756b30426c97f6bb412f997116"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_1-342133e3b8689d17/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d5fd41b4e2d27e8a.arrow\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_1-342133e3b8689d17/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b8d788b32dfe10e8.arrow\n",
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Saving model checkpoint to split-1-hebrew-trc-alephbert-base-EMP\n",
      "Configuration saved in split-1-hebrew-trc-alephbert-base-EMP/config.json\n",
      "Model weights saved in split-1-hebrew-trc-alephbert-base-EMP/pytorch_model.bin\n",
      "tokenizer config file saved in split-1-hebrew-trc-alephbert-base-EMP/tokenizer_config.json\n",
      "Special tokens file saved in split-1-hebrew-trc-alephbert-base-EMP/special_tokens_map.json\n",
      "Found cached dataset csv (/Users/guy.yanko/.cache/huggingface/datasets/csv/split_2-1197cde2a31becd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7376c7caa69448985d9fbc3cb43db4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Assigning ['[א1]', '[/א1]', '[א2]', '[/א2]'] to the additional_special_tokens key of the tokenizer\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_2-1197cde2a31becd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-31b36e8d3cda340f.arrow\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_2-1197cde2a31becd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1acc2ac075a1d719.arrow\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/pytorch_model.bin\n",
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Loading cached shuffled indices for dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_2-1197cde2a31becd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0b3bbd6bd98a5d6b.arrow\n",
      "Saving model checkpoint to split-2-hebrew-trc-alephbert-base-EMP\n",
      "Configuration saved in split-2-hebrew-trc-alephbert-base-EMP/config.json\n",
      "Model weights saved in split-2-hebrew-trc-alephbert-base-EMP/pytorch_model.bin\n",
      "tokenizer config file saved in split-2-hebrew-trc-alephbert-base-EMP/tokenizer_config.json\n",
      "Special tokens file saved in split-2-hebrew-trc-alephbert-base-EMP/special_tokens_map.json\n",
      "Found cached dataset csv (/Users/guy.yanko/.cache/huggingface/datasets/csv/split_3-e8b86e9ba9dd6a4f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a200403cc0aa418580faf3fa418481a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"onlplab/alephbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Assigning ['[א1]', '[/א1]', '[א2]', '[/א2]'] to the additional_special_tokens key of the tokenizer\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_3-e8b86e9ba9dd6a4f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1f28ea1a5d4db429.arrow\n",
      "Loading cached processed dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_3-e8b86e9ba9dd6a4f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-cbed2a77751cdb86.arrow\n",
      "loading configuration file config.json from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/guy.yanko/.cache/huggingface/hub/models--onlplab--alephbert-base/snapshots/1745fb3ff5137e41e9eb4d6246e0758f63b93e46/pytorch_model.bin\n",
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Loading cached shuffled indices for dataset at /Users/guy.yanko/.cache/huggingface/datasets/csv/split_3-e8b86e9ba9dd6a4f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-baa19401e98a124a.arrow\n",
      "Saving model checkpoint to split-3-hebrew-trc-alephbert-base-EMP\n",
      "Configuration saved in split-3-hebrew-trc-alephbert-base-EMP/config.json\n",
      "Model weights saved in split-3-hebrew-trc-alephbert-base-EMP/pytorch_model.bin\n",
      "tokenizer config file saved in split-3-hebrew-trc-alephbert-base-EMP/tokenizer_config.json\n",
      "Special tokens file saved in split-3-hebrew-trc-alephbert-base-EMP/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "for checkpoint in lm_checkpoints:\n",
    "    for arc in architectures:\n",
    "        for split in range(1, 4):\n",
    "            raw_datasets = load_dataset(f\"data_handling/data_splits/split_{split}\")\n",
    "            label2id = {}\n",
    "            id2label = {}\n",
    "            for label, named_label in zip(raw_datasets['train']['label'], raw_datasets['train']['named_label']):\n",
    "                label2id[named_label] = label\n",
    "                id2label[label] = named_label\n",
    "\n",
    "            model_final_name = f'split-{split}-hebrew-trc-{checkpoint.split(\"/\")[1]}-{arc}'\n",
    "            tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "            tokenizer.add_special_tokens({'additional_special_tokens': ['[א1]', '[/א1]', '[א2]', '[/א2]']})\n",
    "            E1_start = tokenizer.convert_tokens_to_ids('[א1]')\n",
    "            E2_start = tokenizer.convert_tokens_to_ids('[א2]')\n",
    "            tokenized_datasets = raw_datasets.map(preprocess_function, remove_columns=['named_label'], batched=True)\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "            tokenizer_class = str(type(tokenizer)).strip(\"><'\").split('.')[-1]\n",
    "            config = TemporalRelationClassificationConfig(special_markers=True,\n",
    "                                                          pool_tokens=True,\n",
    "                                                          EMS1=E1_start,\n",
    "                                                          EMS2=E2_start,\n",
    "                                                          architecture=arc,\n",
    "                                                          num_labels=len(label2id),\n",
    "                                                          id2label=id2label,\n",
    "                                                          label2id=label2id,\n",
    "                                                          name_or_path=checkpoint,\n",
    "                                                          tokenizer_class=tokenizer_class,\n",
    "                                                          vocab_size=len(tokenizer))\n",
    "\n",
    "            model = TemporalRelationClassification(config=config)\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=model_final_name,\n",
    "                learning_rate=2e-5,\n",
    "                per_device_train_batch_size=32,\n",
    "                per_device_eval_batch_size=32,\n",
    "                weight_decay=0.01,\n",
    "                num_train_epochs=10,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                report_to=[],\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_datasets[\"train\"].shuffle(),\n",
    "                eval_dataset=tokenized_datasets[\"test\"].select(),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            trainer.train()\n",
    "            eval_mode = True\n",
    "            print('Evaluate:', model_final_name)\n",
    "            trainer.evaluate(tokenized_datasets['test'])\n",
    "            eval_mode = False\n",
    "            config.register_for_auto_class()\n",
    "            model.register_for_auto_class('AutoModelForSequenceClassification')\n",
    "            # trainer.push_to_hub()\n",
    "            trainer.save_model(model_final_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
