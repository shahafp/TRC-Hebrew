{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "train: 2180\n",
      "test: 756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/545 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/545 [00:00<04:19,  2.10it/s]\u001B[A\n",
      "  0%|          | 2/545 [00:00<03:46,  2.40it/s]\u001B[A\n",
      "  1%|          | 3/545 [00:01<04:16,  2.11it/s]\u001B[A\n",
      "  1%|          | 4/545 [00:01<03:53,  2.31it/s]\u001B[A\n",
      "  1%|          | 5/545 [00:02<03:56,  2.28it/s]\u001B[A\n",
      "  1%|          | 6/545 [00:02<03:53,  2.31it/s]\u001B[A\n",
      "  1%|▏         | 7/545 [00:03<03:56,  2.27it/s]\u001B[A\n",
      "  1%|▏         | 8/545 [00:03<03:45,  2.38it/s]\u001B[A\n",
      "  2%|▏         | 9/545 [00:03<03:48,  2.34it/s]\u001B[A\n",
      "  2%|▏         | 10/545 [00:04<03:43,  2.40it/s]\u001B[A\n",
      "  2%|▏         | 11/545 [00:04<03:42,  2.40it/s]\u001B[A\n",
      "  2%|▏         | 12/545 [00:05<03:29,  2.54it/s]\u001B[A\n",
      "  2%|▏         | 13/545 [00:05<03:31,  2.52it/s]\u001B[A\n",
      "  3%|▎         | 14/545 [00:05<03:22,  2.62it/s]\u001B[A\n",
      "  3%|▎         | 15/545 [00:06<03:27,  2.56it/s]\u001B[A\n",
      "  3%|▎         | 16/545 [00:06<03:11,  2.76it/s]\u001B[A\n",
      "  3%|▎         | 17/545 [00:06<03:04,  2.86it/s]\u001B[A\n",
      "  3%|▎         | 18/545 [00:07<03:05,  2.83it/s]\u001B[A\n",
      "  3%|▎         | 19/545 [00:07<03:19,  2.63it/s]\u001B[A\n",
      "  4%|▎         | 20/545 [00:08<03:41,  2.37it/s]\u001B[A\n",
      "  4%|▍         | 21/545 [00:08<03:37,  2.41it/s]\u001B[A\n",
      "  4%|▍         | 22/545 [00:09<03:42,  2.35it/s]\u001B[A\n",
      "  4%|▍         | 23/545 [00:09<03:41,  2.35it/s]\u001B[A\n",
      "  4%|▍         | 24/545 [00:09<03:33,  2.43it/s]\u001B[A\n",
      "  5%|▍         | 25/545 [00:10<03:26,  2.52it/s]\u001B[A\n",
      "  5%|▍         | 26/545 [00:10<03:50,  2.26it/s]\u001B[A\n",
      "  5%|▍         | 27/545 [00:11<03:33,  2.43it/s]\u001B[A\n",
      "  5%|▌         | 28/545 [00:11<03:35,  2.39it/s]\u001B[A\n",
      "  5%|▌         | 29/545 [00:12<03:51,  2.23it/s]\u001B[A\n",
      "  6%|▌         | 30/545 [00:12<03:58,  2.16it/s]\u001B[A\n",
      "  6%|▌         | 31/545 [00:12<03:44,  2.29it/s]\u001B[A\n",
      "  6%|▌         | 32/545 [00:13<03:29,  2.45it/s]\u001B[A\n",
      "  6%|▌         | 33/545 [00:13<03:34,  2.39it/s]\u001B[A\n",
      "  6%|▌         | 34/545 [00:14<03:42,  2.30it/s]\u001B[A\n",
      "  6%|▋         | 35/545 [00:14<03:22,  2.51it/s]\u001B[A\n",
      "  7%|▋         | 36/545 [00:14<03:22,  2.51it/s]\u001B[A\n",
      "  7%|▋         | 37/545 [00:15<03:07,  2.70it/s]\u001B[A\n",
      "  7%|▋         | 38/545 [00:15<03:06,  2.71it/s]\u001B[A\n",
      "  7%|▋         | 39/545 [00:16<03:33,  2.36it/s]\u001B[A\n",
      "  7%|▋         | 40/545 [00:16<03:31,  2.39it/s]\u001B[A\n",
      "  8%|▊         | 41/545 [00:16<03:06,  2.70it/s]\u001B[A\n",
      "  8%|▊         | 42/545 [00:17<03:18,  2.54it/s]\u001B[A\n",
      "  8%|▊         | 43/545 [00:17<03:39,  2.28it/s]\u001B[A\n",
      "  8%|▊         | 44/545 [00:18<03:19,  2.51it/s]\u001B[A\n",
      "  8%|▊         | 45/545 [00:18<03:17,  2.53it/s]\u001B[A\n",
      "  8%|▊         | 46/545 [00:18<03:08,  2.65it/s]\u001B[A\n",
      "  9%|▊         | 47/545 [00:19<03:12,  2.59it/s]\u001B[A\n",
      "  9%|▉         | 48/545 [00:19<02:59,  2.77it/s]\u001B[A\n",
      "  9%|▉         | 49/545 [00:19<03:15,  2.53it/s]\u001B[A\n",
      "  9%|▉         | 50/545 [00:20<03:40,  2.25it/s]\u001B[A\n",
      "  9%|▉         | 51/545 [00:20<03:20,  2.46it/s]\u001B[A\n",
      " 10%|▉         | 52/545 [00:21<03:21,  2.45it/s]\u001B[A\n",
      " 10%|▉         | 53/545 [00:21<03:10,  2.58it/s]\u001B[A\n",
      " 10%|▉         | 54/545 [00:21<02:58,  2.74it/s]\u001B[A\n",
      " 10%|█         | 55/545 [00:22<03:09,  2.59it/s]\u001B[A\n",
      " 10%|█         | 56/545 [00:22<03:18,  2.46it/s]\u001B[A\n",
      " 10%|█         | 57/545 [00:23<03:02,  2.68it/s]\u001B[A\n",
      " 11%|█         | 58/545 [00:23<03:04,  2.65it/s]\u001B[A\n",
      " 11%|█         | 59/545 [00:23<02:58,  2.72it/s]\u001B[A\n",
      " 11%|█         | 60/545 [00:24<03:30,  2.30it/s]\u001B[A\n",
      " 11%|█         | 61/545 [00:24<03:21,  2.40it/s]\u001B[A\n",
      " 11%|█▏        | 62/545 [00:25<03:43,  2.17it/s]\u001B[A\n",
      " 12%|█▏        | 63/545 [00:25<03:41,  2.18it/s]\u001B[A\n",
      " 12%|█▏        | 64/545 [00:26<03:29,  2.30it/s]\u001B[A\n",
      " 12%|█▏        | 65/545 [00:26<03:12,  2.50it/s]\u001B[A\n",
      " 12%|█▏        | 66/545 [00:26<03:11,  2.51it/s]\u001B[A\n",
      " 12%|█▏        | 67/545 [00:27<03:10,  2.52it/s]\u001B[A\n",
      " 12%|█▏        | 68/545 [00:27<02:51,  2.78it/s]\u001B[A\n",
      " 13%|█▎        | 69/545 [00:27<02:51,  2.78it/s]\u001B[A\n",
      " 13%|█▎        | 70/545 [00:28<02:51,  2.76it/s]\u001B[A\n",
      " 13%|█▎        | 71/545 [00:28<02:50,  2.78it/s]\u001B[A\n",
      " 13%|█▎        | 72/545 [00:29<02:57,  2.66it/s]\u001B[A\n",
      " 13%|█▎        | 73/545 [00:29<02:43,  2.88it/s]\u001B[A\n",
      " 14%|█▎        | 74/545 [00:29<02:40,  2.94it/s]\u001B[A\n",
      " 14%|█▍        | 75/545 [00:29<02:35,  3.03it/s]\u001B[A\n",
      " 14%|█▍        | 76/545 [00:30<02:27,  3.19it/s]\u001B[A\n",
      " 14%|█▍        | 77/545 [00:30<02:34,  3.03it/s]\u001B[A\n",
      " 14%|█▍        | 78/545 [00:30<02:25,  3.22it/s]\u001B[A\n",
      " 14%|█▍        | 79/545 [00:31<02:34,  3.02it/s]\u001B[A\n",
      " 15%|█▍        | 80/545 [00:31<02:48,  2.76it/s]\u001B[A\n",
      " 15%|█▍        | 81/545 [00:32<02:50,  2.73it/s]\u001B[A\n",
      " 15%|█▌        | 82/545 [00:32<02:38,  2.92it/s]\u001B[A\n",
      " 15%|█▌        | 83/545 [00:32<02:52,  2.68it/s]\u001B[A\n",
      " 15%|█▌        | 84/545 [00:33<02:43,  2.82it/s]\u001B[A\n",
      " 16%|█▌        | 85/545 [00:33<02:33,  3.00it/s]\u001B[A\n",
      " 16%|█▌        | 86/545 [00:33<02:58,  2.57it/s]\u001B[A\n",
      " 16%|█▌        | 87/545 [00:34<02:50,  2.68it/s]\u001B[A\n",
      " 16%|█▌        | 88/545 [00:34<02:56,  2.59it/s]\u001B[A\n",
      " 16%|█▋        | 89/545 [00:35<02:55,  2.60it/s]\u001B[A\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim, nn\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from data_handling.trc_dataset import TRCDataset\n",
    "from model.trc_model import TRCModel\n",
    "from trainer.trainer import Trainer\n",
    "from trainer.training_utils import get_parameters\n",
    "\n",
    "if torch.backends.cuda.is_built():\n",
    "    device_name = 'cuda'\n",
    "\n",
    "else:\n",
    "    device_name = 'cpu'\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print('device:', device)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MODEL_CHECKPOINT = 'onlplab/alephbert-base'\n",
    "TRAINING_LAYERS = 52\n",
    "\n",
    "data_paths = {\n",
    "    'train': 'data_handling/split_data/train.csv',\n",
    "    'test': 'data_handling/split_data/test.csv'}\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenizer.add_tokens(['[א1]', '[/א1]', '[א2]', '[/א2]'])\n",
    "E1_start = tokenizer.convert_tokens_to_ids('[א1]')\n",
    "E2_start = tokenizer.convert_tokens_to_ids('[א2]')\n",
    "\n",
    "train_set = TRCDataset(data_path=data_paths['train'])\n",
    "test_set = TRCDataset(data_path=data_paths['test'])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f'train: {len(train_set)}\\ntest: {len(test_set)}')\n",
    "\n",
    "model = TRCModel(output_size=4, tokenizer=tokenizer, check_point=MODEL_CHECKPOINT)\n",
    "\n",
    "trainer = Trainer(model, tokenizer=tokenizer,\n",
    "                  optimizer=optim.Adam(get_parameters(model.named_parameters(), TRAINING_LAYERS), lr=1e-5),\n",
    "                  criterion=nn.CrossEntropyLoss(),\n",
    "                  entity_markers = (E1_start,E2_start),\n",
    "                  device=device)\n",
    "\n",
    "trainer.train(train_loader=train_loader,\n",
    "              test_loader=test_loader,\n",
    "              num_epochs=10,\n",
    "              eval_step=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}